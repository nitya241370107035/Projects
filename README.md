# Machine Learning Projects – Core Algorithms

This repository contains clean, practical implementations of key supervised learning algorithms. Each mini-project demonstrates a different ML technique with real-world-like datasets, clear code, and basic model evaluation.


## 1. Linear Regression

Used for predicting continuous values such as salary, price, etc.

- Goal: Predict outcomes based on a linear relationship.
- Concept: y = mx + c (best-fit line using least squares)


## 2. Logistic Regression

Binary classification algorithm (Yes/No, 0/1)

- Goal: Predict probability of class membership.
- Concept: Sigmoid function (outputs between 0 and 1)


## 3. Support Vector Machine (SVM)

Used for classification with high-dimensional and margin-based data.

- Goal: Find optimal hyperplane to separate classes.
- Concept: Maximize margin between class boundaries


## 4. K-Nearest Neighbors (KNN)

Instance-based learning that uses distance to predict class.

- Goal: Classify a point based on its k nearest neighbors.
- Concept: Euclidean distance, voting among neighbors


## 5. Naive Bayes

Probabilistic classifier based on Bayes’ theorem.

- Goal: Classify text or categorical data (e.g., spam detection).
- Concept: P(class | data) ∝ P(data | class) * P(class)


## 6. Decision Tree

Tree-like structure to split data using logical conditions.

- Goal: Build decision rules from features to classify.
- Concept: Gini Impurity or Entropy used to split nodes


## 7. Random Forest

Ensemble of decision trees that improves model accuracy.

- Goal: Aggregate predictions from multiple decision trees.
- Concept: Bagging + ensemble = more robust predictions


## Tools and Libraries Used

- Python
- Scikit-learn
- NumPy
- Pandas
- Matplotlib / Seaborn
- Jupyter Notebook


## Learning Outcomes

- Gained strong foundational understanding of supervised machine learning algorithms.
- Learned how to split datasets, train models, and evaluate their performance.
- Implemented practical examples with evaluation metrics such as accuracy, precision, recall, and confusion matrix.
